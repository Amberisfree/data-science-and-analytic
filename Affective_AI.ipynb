{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFS45QekQWsf"
   },
   "source": [
    "# **Facial Emotion Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3EB78KyrlA6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhZk6j5GQlNO"
   },
   "source": [
    "## **Problem Definition**\n",
    "\n",
    "**The context:**<br> *Why is this problem important to solve?*<br><br>\n",
    "Affective computing or Emotion AI stands for the study and development of technologies that can read human emotions by means of analyzing body gestures, facial expressions, voice tone and so forth and react accordingly to them.\n",
    "Facial Emotion Recognition (FER) is critical in the fields of human-machine interaction. Recent research has suggested that roughly 50% of communication of sentiments takes place through facial expressions and other visual cues. Hence, training a model to identify facial emotions accurately is an essential step towards the development of emotionally intelligent behaviors in machines with AI capabilities.\n",
    "Some automatic facial expression recognition applications that requires human behaviors understanding include healthcare, branding exposure, customer services, and travel recommendations.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "\n",
    "**The objectives:**<br> *What is the intended goal?*<br><br>\n",
    "\n",
    "This project aims to use Deep Learning and Artificial Intelligence techniques to create a computer vision model that can accurately detect facial emotions. The model should be able to perform multi- class classification on images of facial expressions, to classify the expressions according to the associated emotion.\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "**The key questions:** *What are the key questions that need to be answered?*<br> <br>\n",
    "\n",
    "Accurate Facial Emotion Recognition by computer vision models remain challenging due to the heterogeneity of human faces poses and some naturalistic conditions. Thus, how we build a model with high accuracy becomes the primary purpose throughout the who experimentation.\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "**The problem formulation:** *What are we trying to solve using data science?*\n",
    "<br> <br>\n",
    "\n",
    "Deep learning models, such as CNNs, have shown potential for being able to accurately identify emotions due to their computational efficiency and feature extraction capabilities. These benefits make them good for image classification.\n",
    "We aim to conduct various experiments to explore different ways of optimizing the convolutional neural network, in order to improve its accuracy. For example, we will be trying different optimization algorithms and tuning learning rate schedulers. We found that by thoroughly tuning the model and training hyperparameters, they were able to achieve state-of-the-art results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **About the dataset**\n",
    "\n",
    "The data set consists of 3 folders, i.e., 'test', 'train', and 'validation'. \n",
    "Each of these folders has four subfolders:\n",
    "\n",
    "**‘happy’**: Images of people who have happy facial expressions.<br>\n",
    "**‘sad’**: Images of people with sad or upset facial expressions.<br>\n",
    "**‘surprise’**: Images of people who have shocked or surprised facial expressions.<br>\n",
    "**‘neutral’**: Images of people showing no prominent emotion in their facial expression at all.<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oK2UyYW7Qt2p"
   },
   "source": [
    "## **Mounting the Drive**\n",
    "\n",
    "**NOTE:**  Please use Google Colab from your browser for this notebook. **Google.colab is NOT a library that can be downloaded locally on your device.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEgu2nBKQe3f"
   },
   "outputs": [],
   "source": [
    "# Mounting the drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoqhXzVcQ4aZ"
   },
   "source": [
    "## **Importing the Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5Bn2-gcrhh6"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Importing Deep Learning Libraries\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D, LeakyReLU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxOjwL2QRE1L"
   },
   "source": [
    "### **Let us load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAHXxhdqrrvH"
   },
   "outputs": [],
   "source": [
    "# Storing the path of the data file from the Google drive\n",
    "path = '/content/drive/MyDrive/content/Facial_emotion_images.zip'\n",
    "\n",
    "# The data is provided as a zip file so we need to extract the files from the zip file\n",
    "with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVNVTxfSrsvp"
   },
   "outputs": [],
   "source": [
    "picture_size = 48\n",
    "folder_path = \"Facial_emotion_images/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKraEKeyRREx"
   },
   "source": [
    "## **Visualizing our Classes**\n",
    "\n",
    "Let's look at our classes. \n",
    "\n",
    "**Write down your observation for each class. What do you think can be a unique feature of each emotion, that separates it from the remaining classes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GABUcEciRZS7"
   },
   "source": [
    "### **Happy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhTV4N4PTebB"
   },
   "outputs": [],
   "source": [
    "expression = 'happy'\n",
    "\n",
    "plt.figure(figsize= (8,8))\n",
    "for i in range(1, 10, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "\n",
    "    img = load_img(folder_path + \"train/\" + expression + \"/\" +\n",
    "                  os.listdir(folder_path + \"train/\" + expression)[i], target_size = (picture_size, picture_size))\n",
    "    plt.imshow(img)   \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnbtoV04Tlj9"
   },
   "source": [
    "**Observations and Insights:**\n",
    "Muscle around the eyes tightened, “crows feet” wrinkles around the eyes, cheeks raised, lip corners raised diagonally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVaq2JqpTxzG"
   },
   "source": [
    "### **Sad**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8TyluDTTrMw"
   },
   "outputs": [],
   "source": [
    "# Write your code to visualize images from the class 'sad'.\n",
    "\n",
    "expression = 'sad'\n",
    "\n",
    "plt.figure(figsize= (8,8))\n",
    "for i in range(1, 10, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "\n",
    "    img = load_img(folder_path + \"train/\" + expression + \"/\" +\n",
    "                  os.listdir(folder_path + \"train/\" + expression)[i], target_size = (picture_size, picture_size))\n",
    "    plt.imshow(img)   \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsHoD4gyT2mr"
   },
   "source": [
    "**Observations and Insights:**\n",
    " Drooping eyelids, downcast eyes, lowered lip corners, and slanting inner eyebrows have an arresting effect on observers. However, the social functions of sad expressions are not well understood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4PG5BMdT5qg"
   },
   "source": [
    "### **Neutral**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgYyKVo5UApC"
   },
   "outputs": [],
   "source": [
    "# Write your code to visualize images from the class 'neutral'.\n",
    "\n",
    "expression = 'neutral'\n",
    "\n",
    "plt.figure(figsize= (8,8))\n",
    "for i in range(1, 10, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "\n",
    "    img = load_img(folder_path + \"train/\" + expression + \"/\" +\n",
    "                  os.listdir(folder_path + \"train/\" + expression)[i], target_size = (picture_size, picture_size))\n",
    "    plt.imshow(img)   \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5Ca71nrUBWR"
   },
   "source": [
    "**Observations and Insights:**\n",
    "A neutral face is a blank expression that implies a lack of perceptible emotion. Most of the time an emotionless face is defined by straight-lined mouths, unfocused eyes, and slack cheeks. Though it communicates negativity to some, others see it as a reflection of calmness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaRnvwkrUFhQ"
   },
   "source": [
    "### **Surprised**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vrh4Vw-fUItb"
   },
   "outputs": [],
   "source": [
    "expression = 'surprise'\n",
    "\n",
    "plt.figure(figsize= (8,8))\n",
    "for i in range(1, 10, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "\n",
    "    img = load_img(folder_path + \"train/\" + expression + \"/\" +\n",
    "                  os.listdir(folder_path + \"train/\" + expression)[i], target_size = (picture_size, picture_size))\n",
    "    plt.imshow(img)   \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvoGNGHOUPSz"
   },
   "source": [
    "**Observations and Insights:**\n",
    "In surprise, our eyes are wide open, eyebrows are raised, and jaws drop open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qainkAPDUZIG"
   },
   "source": [
    "## **Checking Distribution of Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzmL7PNjUe_s"
   },
   "outputs": [],
   "source": [
    "# Getting count of images in each folder within our training path\n",
    "num_happy = len(os.listdir(folder_path + \"train/happy\"))\n",
    "print(\"Number of images in the class 'happy':\", num_happy)\n",
    "\n",
    "\n",
    "# Write the code to get the number of training images from the class 'sad'.\n",
    "num_sad = len(os.listdir(folder_path+ \"train/sad\"))\n",
    "print(\"Number of images in the class 'sad':\", num_sad)\n",
    "\n",
    "# Write the code to get the number of training images from the class 'neutral'.\n",
    "num_neutral = len(os.listdir(folder_path+ \"train/neutral\"))\n",
    "print(\"Number of images in the class 'neutral':\", num_neutral)\n",
    "\n",
    "# Write the code to get the number of training images from the class 'surprise'.\n",
    "num_surprise = len(os.listdir(folder_path+ \"train/surprise\"))\n",
    "print(\"Number of images in the class 'surprise':\", num_surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_65suLSbUmz6"
   },
   "outputs": [],
   "source": [
    "# Code to plot histogram\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "data = {'Happy': num_happy, 'Sad': num_sad, 'Neutral': num_neutral, 'Surprise' : num_surprise}\n",
    "\n",
    "df = pd.Series(data)\n",
    "\n",
    "plt.bar(range(len(df)), df.values, align = 'center')\n",
    "\n",
    "plt.xticks(range(len(df)), df.index.values, size = 'small')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PY0yvv1-UqSF"
   },
   "source": [
    "**Observations and Insights:**\n",
    "\n",
    "By checking the distribution of four classes, we observe that the frequency for three categories \"Happy\", \"Sad\", and \"Neutral\" is roughly distributed evenly with approximately 4000 samples. However, the number for the category \"SURPRISE\" is slightly less than the others with only around 3000 samples.This further implies that the imbalance issue is not particularly serious since the distribution across four categories is not drastically uneven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZTJnmGUUtaW"
   },
   "source": [
    "## **Data Augmentation: Creating our Data Loaders**\n",
    "\n",
    "In this section, we are creating data loaders that we will use as inputs to our Neural Network. A sample of the required code has been given with respect to the training data. Please create the data loaders for validation and test set accordingly.\n",
    "\n",
    "**You have two options for the color_mode. You can set it to color_mode = 'rgb' or color_mode = 'grayscale'. You will need to try out both and see for yourself which one gives better performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeeKyOBpreiE"
   },
   "outputs": [],
   "source": [
    "batch_size  = 32\n",
    "img_size = 48\n",
    "\n",
    "datagen_train = ImageDataGenerator(horizontal_flip = True,\n",
    "                                    brightness_range = (0., 2.),\n",
    "                                    rescale = 1./255,\n",
    "                                    shear_range = 0.3)\n",
    "\n",
    "train_set = datagen_train.flow_from_directory(folder_path + \"train\",\n",
    "                                              target_size = (img_size, img_size),\n",
    "                                              color_mode = 'grayscale',\n",
    "                                              batch_size = batch_size,\n",
    "                                              class_mode = 'categorical',\n",
    "                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                              shuffle = True)\n",
    "\n",
    "\n",
    "datagen_validation = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_set = datagen_validation.flow_from_directory(folder_path + \"validation\",\n",
    "                                              target_size = (img_size, img_size),\n",
    "                                              color_mode = 'grayscale',\n",
    "                                              batch_size = batch_size,\n",
    "                                              class_mode = 'categorical',\n",
    "                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                              shuffle = True)\n",
    "\n",
    "datagen_test = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "test_set = datagen_validation.flow_from_directory(folder_path + \"test\",\n",
    "                                              target_size = (img_size, img_size),\n",
    "                                              color_mode = 'grayscale',\n",
    "                                              batch_size = batch_size,\n",
    "                                              class_mode = 'categorical',\n",
    "                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                              shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpctysCLU9bH"
   },
   "source": [
    "## **Model Building**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poHjYkCnVH5-"
   },
   "source": [
    "**Think About It:**\n",
    "* *Are Convolutional Neural Networks the right approach? Should we have gone with Artificial Neural Networks instead?*\n",
    "\n",
    "ANNs and CNNs are both deep learning models. \n",
    "For image datasets, the Convolutional layers work better than fully-connected layers because this kind of filters have the features of locality, which can result in translation invariance while detecting images.\n",
    "However, ANN doesn't have this filter, which means ANN models could potentially see the same feature in differnt positions or from different angles as the differnt things.\n",
    "\n",
    "<br><br>\n",
    "*  *What are the advantages of CNNs over ANNs and are they applicable here?*\n",
    "\n",
    "\n",
    "The greatest benefit of CNN is that we can think of each filter as one particular information dectector. A filter/detector in CNN, as an Object detection algorithm, is used to detect features from all over the image.Therefore, it looks to extract the information from the entire image and is not restricted to a particular region.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6FqDYTkVRSl"
   },
   "source": [
    "### **Creating the Base Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe9ZWYG_VVK1"
   },
   "source": [
    "Our Base Neural network will be a fairly simple model architecture.\n",
    "\n",
    "* We want our Base Neural Network architecture to have 3 convolutional blocks.\n",
    "* Each convolutional block must contain one Conv2D layer followed by a maxpooling layer and one Dropout layer. We can play around with the dropout ratio.\n",
    "* Add first Conv2D layer with **64 filters** and a **kernel size of 2**. Use the 'same' padding and provide the **input_shape = (48, 48, 3) if you are using 'rgb' color mode in your dataloader or else input shape = (48, 48, 1) if you're using 'grayscale' colormode**. Use **'relu' activation**.\n",
    "* Add MaxPooling2D layer with **pool size = 2**.\n",
    "* Add a Dropout layer with a dropout ratio of 0.2.\n",
    "* Add a second Conv2D layer with **32 filters** and a **kernel size of 2**. Use the **'same' padding** and **'relu' activation.**\n",
    "* Follow this up with a similar Maxpooling2D layer like above and a Dropout layer with 0.2 Dropout ratio to complete your second Convolutional Block.\n",
    "* Add a third Conv2D layer with **32 filters** and a **kernel size of 2**. Use the **'same' padding** and **'relu' activation.** Once again, follow it up with a Maxpooling2D layer and a Dropout layer to complete your third Convolutional block.\n",
    "* After adding your convolutional blocks, add your Flatten layer.\n",
    "* Add your first Dense layer with **512 neurons**. Use **'relu' activation function**.\n",
    "* Add a Dropout layer with dropout ratio of 0.4.\n",
    "* Add your final Dense Layer with 4 neurons and **'softmax' activation function**\n",
    "* Print your model summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tN8Rw4b0VYe9"
   },
   "outputs": [],
   "source": [
    "# Initializing a Sequential Model\n",
    "model1 = Sequential()\n",
    "\n",
    "# Add the first Convolutional block\n",
    "model1.add(Conv2D(filters=64,kernel_size=(2,2),activation=\"relu\",padding=\"same\",input_shape=(48,48,1)))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model1.add(Dropout(0.2))\n",
    "\n",
    "# Add the second Convolutional block\n",
    "model1.add(Conv2D(filters=32,kernel_size=(2,2),activation=\"relu\",padding=\"same\"))\n",
    "model1.add(MaxPooling2D(pool_size=2))\n",
    "model1.add(Dropout(0.2))\n",
    "\n",
    "# Add the third Convolutional block\n",
    "model1.add(Conv2D(filters=32,kernel_size=2,activation=\"relu\",padding=\"same\"))\n",
    "model1.add(MaxPooling2D(pool_size=2))\n",
    "model1.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# Add the Flatten layer\n",
    "model1.add(Flatten())\n",
    "\n",
    "# Add the first Dense layer\n",
    "model1.add(Dense(512, activation=\"relu\"))\n",
    "model1.add(Dropout(0.4))\n",
    "\n",
    "# Add the Final layer\n",
    "model1.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6PmyhDLVd6M"
   },
   "source": [
    "### **Compiling and Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbIFfqs_VbZi"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./model1.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss',\n",
    "                          min_delta = 0,\n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True\n",
    "                          )\n",
    "\n",
    "reduce_learningrate = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.2,\n",
    "                              patience = 3,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.0001)\n",
    "\n",
    "callbacks_list = [early_stopping, checkpoint, reduce_learningrate]\n",
    "\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlrLf2_5Vjig"
   },
   "outputs": [],
   "source": [
    "# Write your code to compile your model1. Use categorical crossentropy as your loss function, Adam Optimizer with 0.001 learning rate, and set your metrics to 'accuracy'. \n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "model1.compile(loss=categorical_crossentropy, optimizer=Adam(learning_rate=0.001),metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFJncYzFVmiR"
   },
   "outputs": [],
   "source": [
    "# Write your code to fit your model1. Use train_set as your training data and validation_set as your validation data. Train your model for 20 epochs.\n",
    "import time\n",
    "start=time.time()\n",
    "history = model1.fit(train_set,\n",
    "                     validation_data=validation_set,\n",
    "                     epochs=20)\n",
    "\n",
    "stop=time.time()\n",
    "time=str(start-stop)\n",
    "print(\"model1 requires\"+time+\"training time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkaM_9RTVqD3"
   },
   "outputs": [],
   "source": [
    "# Plotting the accuracies\n",
    "\n",
    "\n",
    "list_ep = [i for i in range(1, 21)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['accuracy'], ls = '--', label = 'accuracy')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_accuracy'], ls = '--', label = 'val_accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLdoYyk_Vsug"
   },
   "outputs": [],
   "source": [
    "# Plotting the loss\n",
    "\n",
    "list_ep = [i for i in range(1, 21)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['loss'], ls = '--', label = 'loss')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_loss'], ls = '--', label = 'val_loss')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Oc7lqTHWDlk"
   },
   "source": [
    "**Observations and Insights:**\n",
    "The epochs history shows that accuracy gradually increases on both training and validation set. However, the overall performance is poor. Additionally, it is a bit rare that the validation accuracy exceed the training accuracy. \n",
    "\n",
    "One possible reason is that the selection of the validation and train data. In this scenario, we don't jnow whether theses two sets selected randomly. It is generally better to have these sets selected randomly from the overall dataset. That way the propability distribution in validation set will closely match the distribution of the training set.\n",
    "\n",
    "\n",
    "The other reason could be the effect of dropout layers. The design for dropout layers is to reduce the problem of overfitting. However, dropout layers in this case actively reduce the accuracy of the training model while they are not active in reducing the validation accuracy. In this case, the overfitting problem doesn't exist, but the underfitting problem appear due to the overuse of dropout layers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZijQyd9cWI1I"
   },
   "source": [
    "### **Evaluating the Model on the Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3p3LNMgWJxO"
   },
   "outputs": [],
   "source": [
    "model1.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYjBgOL9WCy_"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix and generate a classification report for the model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "test_set = datagen_test.flow_from_directory(folder_path + \"test\",\n",
    "                                                              target_size = (img_size,img_size),\n",
    "                                                              color_mode = 'grayscale',\n",
    "                                                              batch_size = 128,\n",
    "                                                              class_mode = 'categorical',\n",
    "                                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                                              shuffle = True) \n",
    "test_images, test_labels = next(test_set)\n",
    "\n",
    "# Write the name of your chosen model in the blank\n",
    "pred = model1.predict(test_images)\n",
    "pred = np.argmax(pred, axis = 1) \n",
    "y_true = np.argmax(test_labels, axis = 1)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(y_true, pred))\n",
    "\n",
    "# Plotting the heatmap using confusion matrix\n",
    "cm = confusion_matrix(y_true, pred)\n",
    "\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize = (8, 5))\n",
    "sns.heatmap(cmn, annot = True,  fmt = '.2f', xticklabels = ['happy', 'sad', 'neutral', 'surprise'], yticklabels = ['happy', 'sad', 'neutral', 'surprise'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp_uxYjPWWPQ"
   },
   "source": [
    "### **Creating the second Convolutional Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r49lSL_SWaca"
   },
   "source": [
    "In the second Neural network, we will add a few more Convolutional blocks. We will also use Batch Normalization layers.\n",
    "\n",
    "* This time, each Convolutional block will have 1 Conv2D layer, followed by a BatchNormalization, LeakuRelU, and a MaxPooling2D layer. We are not adding any Dropout layer this time.\n",
    "* Add first Conv2D layer with **256 filters** and a **kernel size of 2**. Use the 'same' padding and provide the **input_shape = (48, 48, 3) if you are using 'rgb' color mode in your dataloader or else input shape = (48, 48, 1) if you're using 'grayscale' colormode**. Use **'relu' activation**.\n",
    "* Add your BatchNormalization layer followed by a LeakyRelU layer with Leaky ReLU parameter of **0.1**\n",
    "* Add MaxPooling2D layer with **pool size = 2**.\n",
    "* Add a second Conv2D layer with **128 filters** and a **kernel size of 2**. Use the **'same' padding** and **'relu' activation.**\n",
    "* Follow this up with a similar BatchNormalization, LeakyRelU, and Maxpooling2D layer like above to complete your second Convolutional Block.\n",
    "* Add a third Conv2D layer with **64 filters** and a **kernel size of 2**. Use the **'same' padding** and **'relu' activation.** Once again, follow it up with a BatchNormalization, LeakyRelU, and Maxpooling2D layer to complete your third Convolutional block.\n",
    "* Add a fourth block, with the Conv2D layer having **32 filters**.\n",
    "* After adding your convolutional blocks, add your Flatten layer.\n",
    "* Add your first Dense layer with **512 neurons**. Use **'relu' activation function**.\n",
    "* Add the second Dense Layer with **128 neurons** and use **'relu' activation** function.\n",
    "* Add your final Dense Layer with 4 neurons and **'softmax' activation function**\n",
    "* Print your model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWQVVpheWbgS"
   },
   "outputs": [],
   "source": [
    "# Creating sequential model\n",
    "model2 = Sequential()\n",
    " \n",
    "# Add the first Convolutional block\n",
    "model2.add(Conv2D(filters=256,kernel_size=(2,2),padding=\"same\", activation=\"relu\", input_shape=(48,48,1)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(LeakyReLU(0.1))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add the second Convolutional block\n",
    "model2.add(Conv2D(filters=128,kernel_size=(2,2),padding=\"same\", activation=\"relu\"))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(LeakyReLU(0.1))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add the third Convolutional block\n",
    "model2.add(Conv2D(filters=64,kernel_size=(2,2),padding=\"same\", activation=\"relu\"))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(LeakyReLU(0.1))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add the fourth Convolutional block\n",
    "model2.add(Conv2D(filters=32,kernel_size=(2,2),padding=\"same\"))\n",
    "\n",
    "# Add the Flatten layer\n",
    "model2.add(Flatten())\n",
    "\n",
    "# Adding the Dense layers\n",
    "model2.add(Dense(512, activation=\"relu\"))\n",
    "model2.add(Dense(128, activation=\"relu\"))\n",
    "model2.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfAQI4yTWjKV"
   },
   "source": [
    "### **Compiling and Training the Model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UX13Px1VwX_"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./model2.h5\", monitor='val_loss', verbose = 1, save_best_only = True, mode = 'max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\",\n",
    "                               min_delta=0,\n",
    "                               patience=3,\n",
    "                               verbose=1,\n",
    "                               restore_best_weights=True\n",
    "                               ) \n",
    "# Write your code here. You may play around with the hyperparameters if you wish.\n",
    "\n",
    "reduce_learningrate = ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                        factor=0.2,\n",
    "                                        patience=3,\n",
    "                                        verbose=1,\n",
    "                                        min_delta=0.0001)\n",
    "\n",
    "# Write your code here. You may play around with the hyperparameters if you wish.\n",
    "\n",
    "callbacks_list = [early_stopping, checkpoint, reduce_learningrate]\n",
    "\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqvuIQMJWnPT"
   },
   "outputs": [],
   "source": [
    "# Write your code to compile your model2. Use categorical crossentropy as the loss function, Adam Optimizer with 0.001 learning rate, and set metrics as 'accuracy'. \n",
    "\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "\n",
    "model2.compile(loss=categorical_crossentropy, optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "966p69uGWs1b"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start=time.time()\n",
    "history = model2.fit(train_set,\n",
    "                     validation_data=validation_set,\n",
    "                     epochs=20)\n",
    "stop=time.time()\n",
    "time=str(stop-start)\n",
    "\n",
    "print(\"model2 requires \" +time+ \" training time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8ASNJXCWt9S"
   },
   "outputs": [],
   "source": [
    "list_ep = [i for i in range(1, 21)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['accuracy'], ls = '--', label = 'accuracy')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_accuracy'], ls = '--', label = 'val_accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WbctkwiXGK3"
   },
   "source": [
    "### **Evaluating the Model on the Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbiJz7mJXKov"
   },
   "outputs": [],
   "source": [
    "model2.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcqBva2DXPKD"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix and generate a classification report for the model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "test_set = datagen_test.flow_from_directory(folder_path + \"test\",\n",
    "                                                              target_size = (img_size,img_size),\n",
    "                                                              color_mode = 'grayscale',\n",
    "                                                              batch_size = 128,\n",
    "                                                              class_mode = 'categorical',\n",
    "                                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                                              shuffle = True) \n",
    "test_images, test_labels = next(test_set)\n",
    "\n",
    "# Write the name of your chosen model in the blank\n",
    "pred = model2.predict(test_images)\n",
    "pred = np.argmax(pred, axis = 1) \n",
    "y_true = np.argmax(test_labels, axis = 1)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(y_true, pred))\n",
    "\n",
    "# Plotting the heatmap using confusion matrix\n",
    "cm = confusion_matrix(y_true, pred)\n",
    "\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize = (8, 5))\n",
    "sns.heatmap(cmn, annot = True,  fmt = '.2f', xticklabels = ['happy', 'sad', 'neutral', 'surprise'], yticklabels = ['happy', 'sad', 'neutral', 'surprise'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uHHk_c6XmTV"
   },
   "source": [
    "### <u>**Proposed Approach**</u>\n",
    "\n",
    "- **Potential techniques:** What different techniques should be explored ?<br>\n",
    "\n",
    "For image classification problem, we will explore supervised machine learning models to train. Specifically, we intend to use Deep learning CNN architecture to solve classification problem and extract features.\n",
    "Additionally, it is generally hard to collect many images and then train CNNs. In that case, we can take advantage of data Augmentation/ image data generator since CNNs have the property of translational invariance\n",
    "Most importantly, we will try different techniques that can be used to improve the performance of a machine learning model. The Sigmoid activation function is a mathematical equation that is used to determine the output of a neural network. It has been replaced by the Rectified Linear Unit (ReLU) activation to help speed up training and avoid problems with the gradient. Pooling is a technique used to reduce the size of the input data and help the model generalize better. Dropout, regularization, and data augmentation are all techniques used to prevent the model from overfitting, which is when the model performs well on the training data but not on new data.Batch\n",
    "normalization is a technique used to help prevent the gradient from vanishing or exploding, which can cause the model to not learn properly.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Overall solution design:** What is the potential solution design?<br>\n",
    "\n",
    "\n",
    "In this experiment, we will be running two variations of CNN models to compare their performance on the dataset.Also, this experiment will be conducted in 3 stages.\n",
    "The major difference of two models is complexity in their depth and their width. Another difference is their approach to deal with gradient descent. One is a base CNN model with less filters and some Dropout layers to reduce overfitting problems while the other model is more sophisticated with more filters for each convolutional layer followed by one additional Dense layer. That is, the second neural network is deeper and wider, which means the model has more neurons to train. Moreover, as the second model get deeper, the model adopt Batch Normalization to accelerate and stabilize deep learning training. The second model also has LeakyReLU as an improved ReLU activation.\n",
    "The process of experimentation consists of 5 steps. The first step is data pre-processing, followed by data augmentation to increase diversity of the data. Secondly, we will train two different models. Thirdly, these two models will be compiled by the same optimizer using Adam and the same loss function of cross entropy.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Measures of success:** What are the key measures of success to compare different techniques?<br>\n",
    "\n",
    "Aside from plotting accuracy for both training and validation dataset, the measures of success in image classification depends on the rate for misclassification in test data. Here, we will introduce confusion matrix. Under this hood, precision, recall, f1-score, and so forth will determine which model works better in terms of misclassification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "throMsS9XocS"
   },
   "source": [
    "## **Transfer Learning Architectures**\n",
    "\n",
    "In this section, we will create several Transfer Learning architectures. For the pre-trained models, we will select three popular architectures namely, VGG16, ResNet v2, and Efficient Net. The difference between these architectures and the previous architectures is that these will require 3 input channels while the earlier ones worked on 'grayscale' images. Therefore, we need to create new DataLoaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbwPvUopYC31"
   },
   "source": [
    "### **Creating our Data Loaders for Transfer Learning Architectures**\n",
    "\n",
    "In this section, we are creating data loaders that we will use as inputs to our Neural Network. Unlike in Milestone 1, we will have to go with color_mode = 'rgb' as this is the required format for the transfer learning architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjzeWT3NX-eM"
   },
   "outputs": [],
   "source": [
    "batch_size  = 32\n",
    "img_size = 48\n",
    "\n",
    "datagen_train = ImageDataGenerator(horizontal_flip = True,\n",
    "                                    brightness_range = (0., 2.),\n",
    "                                    rescale = 1./255,\n",
    "                                    shear_range = 0.3)\n",
    "\n",
    "train_set = datagen_train.flow_from_directory(folder_path + \"train\",\n",
    "                                              target_size = (img_size, img_size),\n",
    "                                              color_mode = 'rgb',\n",
    "                                              batch_size = batch_size,\n",
    "                                              class_mode = 'categorical',\n",
    "                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                              shuffle = True)\n",
    "\n",
    "datagen_validation = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "validation_set = datagen_validation.flow_from_directory(folder_path + \"validation\",\n",
    "                                              target_size = (img_size, img_size),\n",
    "                                              color_mode = 'rgb',\n",
    "                                              batch_size = batch_size,\n",
    "                                              class_mode = 'categorical',\n",
    "                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                              shuffle = True)\n",
    "\n",
    "datagen_test = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_set = datagen_test.flow_from_directory(folder_path + \"test\",\n",
    "                                              target_size = (img_size, img_size),\n",
    "                                              color_mode = 'rgb',\n",
    "                                              batch_size = batch_size,\n",
    "                                              class_mode = 'categorical',\n",
    "                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                              shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwhsSCESYMrw"
   },
   "source": [
    "## **VGG16 Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOBRSXcDYNTU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "vgg = VGG16(include_top = False, weights = 'imagenet', input_shape = (48, 48, 3))\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3k6D_GLYVb4"
   },
   "source": [
    "### **Model Building**\n",
    "\n",
    "* In this model, we will import till the **'block5_pool'** layer of the VGG16 model. You can scroll down in the model summary and look for 'block5_pool'. You can choose any other layer as well.\n",
    "* Then we will add a Flatten layer, which receives the output of the 'block5_pool' layer as its input.\n",
    "* We will add a few Dense layers and use 'relu' activation function on them.\n",
    "* You may use Dropout and BatchNormalization layers as well.\n",
    "* Then we will add our last dense layer, which must have 4 neurons and a 'softmax' activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJRDaDmEYIUt"
   },
   "outputs": [],
   "source": [
    "transfer_layer = vgg.get_layer('block5_pool')\n",
    "vgg.trainable = False\n",
    "number=0\n",
    "for layer in vgg.layers:\n",
    "    #print(layer.name, layer.trainable)\n",
    "    number+=1\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmwWgUmKYb-M"
   },
   "outputs": [],
   "source": [
    "transfer_layer = vgg.get_layer('block5_pool')\n",
    "vgg.trainable = False\n",
    "\n",
    "# Add classification layers on top of it  \n",
    "x=Sequential()\n",
    "x.add(vgg)\n",
    "\n",
    "\n",
    "\n",
    "# Flattenning the output from the 3rd block of the VGG16 model\n",
    "x = Flatten()(transfer_layer.output)\n",
    "\n",
    "\n",
    "\n",
    "# Adding a Dense layer with 256 neurons\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "\n",
    "# Add a Dense Layer with 128 neurons\n",
    "x =Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "# Add a DropOut layer with Drop out ratio of 0.3\n",
    "x =Dropout(0.3)(x)\n",
    "\n",
    "# Add a Dense Layer with 64 neurons\n",
    "x =Dense(64, activation=\"relu\")(x)\n",
    "\n",
    "# Add a Batch Normalization layer\n",
    "x =BatchNormalization()(x)\n",
    "\n",
    "# Adding the final dense layer with 4 neurons and use 'softmax' activation\n",
    "pred = Dense(4, activation='softmax')(x)\n",
    "\n",
    "vggmodel = Model(vgg.input, pred) # Initializing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTPTNvxuYonW"
   },
   "source": [
    "### **Compiling and Training the VGG16 Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBbhE47mYqDR"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./vggmodel.h5\", monitor = 'val_loss', verbose = 1, save_best_only = True, mode = 'max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss',\n",
    "                          min_delta = 0,\n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True\n",
    "                          )\n",
    "\n",
    "reduce_learningrate = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.2,\n",
    "                              patience = 3,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.0001)\n",
    "\n",
    "callbacks_list = [early_stopping, checkpoint, reduce_learningrate]\n",
    "\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmNjzGhAYsM8"
   },
   "outputs": [],
   "source": [
    "# Write your code to compile the vggmodel. Use categorical crossentropy as the loss function, Adam Optimizer with 0.001 learning rate, and set metrics to 'accuracy'. \n",
    "vggmodel.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "vggmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpVV3v5NZERv"
   },
   "outputs": [],
   "source": [
    "# Write your code to fit your model. Use train_set as the training data and validation_set as the validation data. Train the model for 20 epochs.\n",
    "import time\n",
    "start=time.time()\n",
    "history = vggmodel.fit(train_set,\n",
    "                       validation_data=validation_set,\n",
    "                       epochs=20)\n",
    "stop=time.time()\n",
    "\n",
    "time=str(stop-start)\n",
    "\n",
    "print(\"VGG16 model requires \" +time+ \" training time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSgHJHppZMGE"
   },
   "outputs": [],
   "source": [
    "list_ep = [i for i in range(1, 21)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['accuracy'], ls = '--', label = 'accuracy')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_accuracy'], ls = '--', label = 'val_accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbxLeTgfZSwB"
   },
   "outputs": [],
   "source": [
    "list_ep = [i for i in range(1, 21)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['loss'], ls = '--', label = 'loss')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_loss'], ls = '--', label = 'val_loss')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lU_wo5vBZdXs"
   },
   "source": [
    "### **Evaluating the VGG16 model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hwEXboOZhMG"
   },
   "outputs": [],
   "source": [
    "# Write your code to evaluate model performance on the test set\n",
    "vggmodel.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tvg0HyUSZjY7"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix and generate a classification report for the model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "test_set = datagen_test.flow_from_directory(folder_path + \"test\",\n",
    "                                                              target_size = (img_size,img_size),\n",
    "                                                              color_mode = 'rgb',\n",
    "                                                              batch_size = 128,\n",
    "                                                              class_mode = 'categorical',\n",
    "                                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                                              shuffle = True) \n",
    "test_images, test_labels = next(test_set)\n",
    "\n",
    "# Write the name of your chosen model in the blank\n",
    "pred = vggmodel.predict(test_images)\n",
    "pred = np.argmax(pred, axis = 1) \n",
    "y_true = np.argmax(test_labels, axis = 1)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(y_true, pred))\n",
    "\n",
    "# Plotting the heatmap using confusion matrix\n",
    "cm = confusion_matrix(y_true, pred)\n",
    "\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize = (8, 5))\n",
    "sns.heatmap(cmn, annot = True,  fmt = '.2f', xticklabels = ['happy', 'sad', 'neutral', 'surprise'], yticklabels = ['happy', 'sad', 'neutral', 'surprise'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3v-5oykZt2G"
   },
   "source": [
    "**Observations and Insights:**\n",
    "\n",
    "Even though training and validation accuracy are  consistently improving as we run more epoch, both accuracy still seems low.  The test accuracy achieved 0.57."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvokAcNFZ3Z7"
   },
   "source": [
    "## **ResNet V2 Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YYnyyEhZoZj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.applications as ap\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "Resnet = ap.ResNet101(include_top = False, weights = \"imagenet\", input_shape=(48,48,3))\n",
    "Resnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBxoyAdEaAye"
   },
   "source": [
    "### **Model Building**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "to34DNPtaG25"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TD4jFmNsZ9gt"
   },
   "outputs": [],
   "source": [
    "transfer_layer_Resnet = Resnet.get_layer('conv5_block3_add')\n",
    "Resnet.trainable=False\n",
    "number=0\n",
    "for layer in Resnet.layers:\n",
    "    #print(layer.name, layer.trainable)\n",
    "    number+=1\n",
    "number=str(number)\n",
    "print(\"Resnet has \" +number+ \"layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-6LDav9Z9yN"
   },
   "outputs": [],
   "source": [
    "transfer_layer_Resnet = Resnet.get_layer('conv5_block3_add')\n",
    "Resnet.trainable=False\n",
    "\n",
    "# Add classification layers on top of it\n",
    "x=Sequential()\n",
    "x.add(Resnet)\n",
    "# Flattenning the output from the 3rd block of the VGG16 model\n",
    "x = Flatten()(transfer_layer_Resnet.output)\n",
    "\n",
    "# Add a Dense layer with 256 neurons\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "\n",
    "# Add a Dense Layer with 128 neurons\n",
    "x =Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "# Add a DropOut layer with Drop out ratio of 0.3\n",
    "x= Dropout(0.3)(x)\n",
    "\n",
    "# Add a Dense Layer with 64 neurons\n",
    "x =Dense(64, activation=\"relu\")(x)\n",
    "\n",
    "# Add a Batch Normalization layer\n",
    "x= BatchNormalization()(x)\n",
    "\n",
    "# Add the final dense layer with 4 neurons and use a 'softmax' activation\n",
    "pred = Dense(4, activation = 'softmax')(x)\n",
    "\n",
    "resnetmodel = Model(Resnet.input, pred) # Initializing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evipxkSgaOe9"
   },
   "source": [
    "### **Compiling and Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jO7swpxvZ94M"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./Resnetmodel.h5\", monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'max')\n",
    "\n",
    "# Write your code here. You may play around with the hyperparameters if you wish.\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss',\n",
    "                          min_delta = 0,\n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True\n",
    "                          )\n",
    "\n",
    "# Write your code here. You may play around with the hyperparameters if you wish.\n",
    "\n",
    "reduce_learningrate =  ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.2,\n",
    "                              patience = 3,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.0001)\n",
    "\n",
    "\n",
    "callbacks_list = [early_stopping, checkpoint, reduce_learningrate]\n",
    "\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAZbuDQJZ98i"
   },
   "outputs": [],
   "source": [
    "# Write your code to compile your resnetmodel. Use categorical crossentropy as your loss function, Adam Optimizer with 0.001 learning rate, and set your metrics to 'accuracy'. \n",
    "\n",
    "resnetmodel.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "resnetmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vq2g_cPZ9_6"
   },
   "outputs": [],
   "source": [
    " # Write your code to fit your model. Use train_set as your training data and validation_set as your validation data. Train your model for 20 epochs.\n",
    "\n",
    "# Write your code to fit your model. Use train_set as your training data and validation_set as your validation data. Train your model for 20 epochs.\n",
    "import time\n",
    "start=time.time()\n",
    "history = resnetmodel.fit(train_set,\n",
    "                       validation_data=validation_set,\n",
    "                       epochs=20)\n",
    "stop=time.time()\n",
    "time=str(stop-start)\n",
    "\n",
    "print(\"ResNet101 model requires \" +time+ \" training time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozIWsQznZ-C3"
   },
   "outputs": [],
   "source": [
    "list_ep = [i for i in range(1, 21)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['accuracy'], ls = '--', label = 'accuracy')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_accuracy'], ls = '--', label = 'val_accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XabDIu5aahkY"
   },
   "outputs": [],
   "source": [
    "list_ep = [i for i in range(1, 21)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['loss'], ls = '--', label = 'loss')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_loss'], ls = '--', label = 'val_loss')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bD_pMN65arvZ"
   },
   "source": [
    "### **Evaluating the ResNet Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8Qu69MNatCL"
   },
   "outputs": [],
   "source": [
    "# Write your code to evaluate model performance on the test set\n",
    "resnetmodel.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DecHD5fLawHy"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix and generate a classification report for the model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "test_set = datagen_test.flow_from_directory(folder_path + \"test\",\n",
    "                                                              target_size = (img_size,img_size),\n",
    "                                                              color_mode = 'rgb',\n",
    "                                                              batch_size = 128,\n",
    "                                                              class_mode = 'categorical',\n",
    "                                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                                              shuffle = True) \n",
    "test_images, test_labels = next(test_set)\n",
    "\n",
    "# Write the name of your chosen model in the blank\n",
    "pred = resnetmodel.predict(test_images)\n",
    "pred = np.argmax(pred, axis = 1) \n",
    "y_true = np.argmax(test_labels, axis = 1)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(y_true, pred))\n",
    "\n",
    "# Plotting the heatmap using confusion matrix\n",
    "cm = confusion_matrix(y_true, pred)\n",
    "\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize = (8, 5))\n",
    "sns.heatmap(cmn, annot = True,  fmt = '.2f', xticklabels = ['happy', 'sad', 'neutral', 'surprise'], yticklabels = ['happy', 'sad', 'neutral', 'surprise'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a73zumGXa2Wu"
   },
   "source": [
    "## **EfficientNet Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNn98KQoa10o"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.applications as ap\n",
    "from tensorflow.keras import Model\n",
    "EfficientNet = ap.EfficientNetV2B2(include_top=False,weights=\"imagenet\", input_shape= (48, 48, 3))\n",
    "\n",
    "EfficientNet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBC8irQ1bC1M"
   },
   "source": [
    "### **Model Building**\n",
    "\n",
    "**Build your own Architecture on top of the transfer layer. Be sure to have a Flatten layer after your transfer layer and also make sure you have 4 neurons and softmax activation function in your last dense layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJiIooAVbKWM"
   },
   "outputs": [],
   "source": [
    "transfer_layer_EfficientNet = EfficientNet.get_layer('block6e_expand_activation')\n",
    "EfficientNet.trainable = False\n",
    "\n",
    "x=Sequential()\n",
    "x.add(transfer_layer_EfficientNet)\n",
    "# Add your Flatten layer.\n",
    "x=Flatten()(transfer_layer_EfficientNet.output)\n",
    "\n",
    "# Add your Dense layers and/or BatchNormalization and Dropout layers\n",
    "x=Dense(256, activation=\"relu\")(x)\n",
    "x=BatchNormalization()(x)\n",
    "\n",
    "x=Dense(128, activation=\"relu\")(x)\n",
    "x=BatchNormalization()(x)\n",
    "# Add your final Dense layer with 4 neurons and softmax activation function.\n",
    "\n",
    "pred=Dense(4, activation=\"softmax\")(x)\n",
    "\n",
    "Efficientnetmodel = Model(EfficientNet.input, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWXF-L0GbMmY"
   },
   "source": [
    "### **Compiling and Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-Vq9idzbORP"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./Efficientnetmodel.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss',\n",
    "                          min_delta = 0,\n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True\n",
    "                          )\n",
    "\n",
    "reduce_learningrate = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.2,\n",
    "                              patience = 3,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.0001)\n",
    "callbacks_list = [early_stopping,checkpoint,reduce_learningrate]\n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_Lpx22sbQJ5"
   },
   "outputs": [],
   "source": [
    "# Write your code to compile your Efficientnetmodel. Use categorical crossentropy as your loss function, Adam Optimizer with 0.001 learning rate, and set your metrics to 'accuracy'.\n",
    "\n",
    "Efficientnetmodel.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "Efficientnetmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0winWTwSbUMS"
   },
   "outputs": [],
   "source": [
    "# Write your code to fit your model. Use train_set as your training data and validation_set as your validation data. Train your model for 20 epochs.\n",
    "import time\n",
    "start=time.time()\n",
    "history = Efficientnetmodel.fit(train_set,\n",
    "                       validation_data=validation_set,\n",
    "                       epochs=20)\n",
    "stop=time.time()\n",
    "time=str(stop-start)\n",
    "\n",
    "print(\"EfficientNet model requires \" +time+ \" training time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHWOEwVuboA0"
   },
   "outputs": [],
   "source": [
    "list_ep = [i for i in range(1, 21)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['accuracy'], ls = '--', label = 'accuracy')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_accuracy'], ls = '--', label = 'val_accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7zvAUTsbxXy"
   },
   "outputs": [],
   "source": [
    "list_ep = [i for i in range(1, 21)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['loss'], ls = '--', label = 'loss')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_loss'], ls = '--', label = 'val_loss')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eu8LsG2bhM7"
   },
   "source": [
    "### **Evaluating the EfficientnetNet Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3K-HNBVmbeMH"
   },
   "outputs": [],
   "source": [
    "# Write your code to evaluate the model performance on the test set\n",
    "\n",
    "Efficientnetmodel.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPSijfk1cFol"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix and generate a classification report for the model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "test_set = datagen_test.flow_from_directory(folder_path + \"test\",\n",
    "                                                              target_size = (img_size,img_size),\n",
    "                                                              color_mode = 'rgb',\n",
    "                                                              batch_size = 128,\n",
    "                                                              class_mode = 'categorical',\n",
    "                                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                                              shuffle = True) \n",
    "test_images, test_labels = next(test_set)\n",
    "\n",
    "# Write the name of your chosen model in the blank\n",
    "pred = resnetmodel.predict(test_images)\n",
    "pred = np.argmax(pred, axis = 1) \n",
    "y_true = np.argmax(test_labels, axis = 1)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(y_true, pred))\n",
    "\n",
    "# Plotting the heatmap using confusion matrix\n",
    "cm = confusion_matrix(y_true, pred)\n",
    "\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize = (8, 5))\n",
    "sns.heatmap(cmn, annot = True,  fmt = '.2f', xticklabels = ['happy', 'sad', 'neutral', 'surprise'], yticklabels = ['happy', 'sad', 'neutral', 'surprise'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqYAIy6AcYix"
   },
   "source": [
    "## **Building a Complex Neural Network Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0HIjetZcksH"
   },
   "source": [
    "In this section, we will build a more complex Convolutional Neural Network Model that has close to as many parameters as we had in our Transfer Learning Models. However, we will have only 1 input channel for our input images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOtS26Fxdie3"
   },
   "source": [
    "## **Creating our Data Loaders**\n",
    "\n",
    "In this section, we are creating data loaders which we will use as inputs to the more Complicated Convolutional Neural Network. We will go ahead with color_mode = 'grayscale'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghohg6endf7W"
   },
   "outputs": [],
   "source": [
    "batch_size  = 32\n",
    "img_size = 48\n",
    "\n",
    "datagen_train = ImageDataGenerator(horizontal_flip = True,\n",
    "                                    brightness_range = (0., 2.),\n",
    "                                    rescale = 1./255,\n",
    "                                    shear_range = 0.3)\n",
    "\n",
    "train_set = datagen_train.flow_from_directory(folder_path + \"train\",\n",
    "                                              target_size = (img_size, img_size),\n",
    "                                              color_mode = 'grayscale',\n",
    "                                              batch_size = batch_size,\n",
    "                                              class_mode = 'categorical',\n",
    "                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                              shuffle = True)\n",
    "\n",
    "\n",
    "datagen_validation = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_set = datagen_validation.flow_from_directory(folder_path + \"validation\",\n",
    "                                              target_size = (img_size, img_size),\n",
    "                                              color_mode = 'grayscale',\n",
    "                                              batch_size = batch_size,\n",
    "                                              class_mode = 'categorical',\n",
    "                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                              shuffle = True)\n",
    "\n",
    "datagen_test = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "test_set = datagen_validation.flow_from_directory(folder_path + \"test\",\n",
    "                                              target_size = (img_size, img_size),\n",
    "                                              color_mode = 'grayscale',\n",
    "                                              batch_size = batch_size,\n",
    "                                              class_mode = 'categorical',\n",
    "                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                              shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEKk3iCmdq4g"
   },
   "source": [
    "### **Model Building**\n",
    "\n",
    "* In this network, we plan to have 5 Convolutional Blocks\n",
    "* Add first Conv2D layer with **64 filters** and a **kernel size of 2**. Use the 'same' padding and provide the **input shape = (48, 48, 1)**. Use **'relu' activation**.\n",
    "* Add your BatchNormalization layer followed by a LeakyRelU layer with Leaky ReLU parameter of **0.1**\n",
    "* Add MaxPooling2D layer with **pool size = 2**.\n",
    "* Add a Dropout layer with a Dropout Ratio of **0.2**. This completes the first Convolutional block.\n",
    "* Add a second Conv2D layer with **128 filters** and a **kernel size of 2**. Use the **'same' padding** and **'relu' activation.**\n",
    "* Follow this up with a similar BatchNormalization, LeakyRelU, Maxpooling2D, and Dropout layer like above to complete your second Convolutional Block.\n",
    "* Add a third Conv2D layer with **512 filters** and a **kernel size of 2**. Use the **'same' padding** and **'relu' activation.** Once again, follow it up with a BatchNormalization, LeakyRelU, Maxpooling2D, and Dropout layer to complete your third Convolutional block.\n",
    "* Add a fourth block, with the Conv2D layer having **512 filters**.\n",
    "* Add the fifth block, having **128 filters**.\n",
    "* Then add your Flatten layer, followed by your Dense layers.\n",
    "* Add your first Dense layer with **256 neurons** followed by a BatchNormalization layer, a **'relu'** Activation, and a Dropout layer. This forms your first Fully Connected block\n",
    "* Add your second Dense layer with **512 neurons**, again followed by a BatchNormalization layer, **relu** activation, and a Dropout layer.\n",
    "* Add your final Dense layer with 4 neurons.\n",
    "* Compile your model with the optimizer of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gC3pGFirS8e"
   },
   "outputs": [],
   "source": [
    "no_of_classes = 4\n",
    "  \n",
    "model3 = Sequential()\n",
    "\n",
    "# Add 1st CNN Block\n",
    "model3.add(Conv2D(64, (2,2), activation='relu', input_shape=(48, 48, 1), padding = 'same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(LeakyReLU(0.1))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# Add 2nd CNN Block\n",
    "model3.add(Conv2D(128, (2,2), activation='relu', padding = 'same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(LeakyReLU(0.1))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "# Add 3rd CNN Block\n",
    "model3.add(Conv2D(512, (2,2), activation='relu', padding = 'same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(LeakyReLU(0.1))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# Add 4th CNN Block\n",
    "model3.add(Conv2D(512, (2,2), activation='relu', padding = 'same'))\n",
    "\n",
    "# Add 5th CNN Block\n",
    "model3.add(Conv2D(128, (2,2), activation='relu', padding = 'same'))\n",
    "\n",
    "\n",
    "model3.add(Flatten())\n",
    "\n",
    "# First fully connected layer\n",
    "model3.add(Dense(256, activation=\"relu\"))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "# Second fully connected layer\n",
    "model3.add(Dense(512, activation=\"relu\"))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "model3.add(Dense(no_of_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wG7RUkAvd6iP"
   },
   "source": [
    "### **Compiling and Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWzwvSJZd1wA"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "epochs = 35\n",
    "\n",
    "steps_per_epoch = train_set.n//train_set.batch_size\n",
    "validation_steps = validation_set.n//validation_set.batch_size\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model3.h5\", monitor = 'val_accuracy',\n",
    "                            save_weights_only = True, model = 'max', verbose = 1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 2, min_lr = 0.0001 , model = 'auto')\n",
    "\n",
    "callbacks = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-VX_h0ieGqL"
   },
   "outputs": [],
   "source": [
    "# Write your code to compile your model3. Use categorical crossentropy as the loss function, Adam Optimizer with 0.003 learning rate, and set metrics to 'accuracy'.\n",
    "model3.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.003), metrics=[\"accuracy\"])\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCucgvSKsALM"
   },
   "outputs": [],
   "source": [
    "# Write your code to fit your model. Use train_set as the training data and validation_set as the validation data. Train your model for 35 epochs.\n",
    "import time\n",
    "start=time.time()\n",
    "history = model3.fit(train_set,\n",
    "                     validation_data=validation_set,\n",
    "                     epochs=35)\n",
    "\n",
    "stop=time.time()\n",
    "time=str(stop-start)\n",
    "\n",
    "print(\"model3 requires \" +time+ \" training time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9u_A-PKsLS8"
   },
   "outputs": [],
   "source": [
    "list_ep = [i for i in range(1, 36)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['accuracy'], ls = '--', label = 'accuracy')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_accuracy'], ls = '--', label = 'val_accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.title(\"Model 3 History\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pivW0SwLfNjS"
   },
   "outputs": [],
   "source": [
    "list_ep = [i for i in range(1, 36)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['loss'], ls = '--', label = 'loss')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_loss'], ls = '--', label = 'val_loss')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.title(\"Model 3 History\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7vwAVHde-US"
   },
   "source": [
    "### **Evaluating the Model on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8g5h6j7sPNG"
   },
   "outputs": [],
   "source": [
    "model3.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAL2lfbow8YU"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix and generate a classification report for the model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "test_set = datagen_test.flow_from_directory(folder_path + \"test\",\n",
    "                                                              target_size = (img_size,img_size),\n",
    "                                                              color_mode = 'grayscale',\n",
    "                                                              batch_size = 128,\n",
    "                                                              class_mode = 'categorical',\n",
    "                                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                                              shuffle = True) \n",
    "test_images, test_labels = next(test_set)\n",
    "\n",
    "# Write the name of your chosen model in the blank\n",
    "pred = model3.predict(test_images)\n",
    "pred = np.argmax(pred, axis = 1) \n",
    "y_true = np.argmax(test_labels, axis = 1)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(y_true, pred))\n",
    "\n",
    "# Plotting the heatmap using confusion matrix\n",
    "cm = confusion_matrix(y_true, pred)\n",
    "\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize = (8, 5))\n",
    "sns.heatmap(cmn, annot = True,  fmt = '.2f', xticklabels = ['happy', 'sad', 'neutral', 'surprise'], yticklabels = ['happy', 'sad', 'neutral', 'surprise'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcPuvtkcfkrz"
   },
   "source": [
    "## **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad05tkX-fwcm"
   },
   "source": [
    "### **kernel size (3,3), learning rates 0.003**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwEciJMNgnUG"
   },
   "outputs": [],
   "source": [
    "no_of_classes = 4\n",
    "  \n",
    "model3 = Sequential()\n",
    "\n",
    "# Add 1st CNN Block\n",
    "model3.add(Conv2D(64, (3,3), activation='relu', input_shape=(48, 48, 1), padding = 'same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(LeakyReLU(0.1))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# Add 2nd CNN Block\n",
    "model3.add(Conv2D(128, (3,3), activation='relu', padding = 'same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(LeakyReLU(0.1))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "# Add 3rd CNN Block\n",
    "model3.add(Conv2D(512, (3,3), activation='relu', padding = 'same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(LeakyReLU(0.1))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# Add 4th CNN Block\n",
    "model3.add(Conv2D(512, (3,3), activation='relu', padding = 'same'))\n",
    "\n",
    "# Add 5th CNN Block\n",
    "model3.add(Conv2D(128, (3,3), activation='relu', padding = 'same'))\n",
    "\n",
    "\n",
    "model3.add(Flatten())\n",
    "\n",
    "# First fully connected layer\n",
    "model3.add(Dense(256, activation=\"relu\"))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "# Second fully connected layer\n",
    "model3.add(Dense(512, activation=\"relu\"))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "model3.add(Dense(no_of_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSKll3sTinqu"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "epochs = 35\n",
    "\n",
    "steps_per_epoch = train_set.n//train_set.batch_size\n",
    "validation_steps = validation_set.n//validation_set.batch_size\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model3.h5\", monitor = 'val_accuracy',\n",
    "                            save_weights_only = True, model = 'max', verbose = 1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 2, min_lr = 0.0001 , model = 'auto')\n",
    "\n",
    "callbacks = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iojqp7WVioi3"
   },
   "outputs": [],
   "source": [
    "# Write your code to compile your model3. Use categorical crossentropy as the loss function, Adam Optimizer with 0.003 learning rate, and set metrics to 'accuracy'.\n",
    "model3.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.003), metrics=[\"accuracy\"])\n",
    "\n",
    "model3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5Wz9cC3iteN"
   },
   "outputs": [],
   "source": [
    "# Write your code to fit your model. Use train_set as the training data and validation_set as the validation data. Train your model for 35 epochs.\n",
    "import time\n",
    "start=time.time()\n",
    "history = model3.fit(train_set,\n",
    "                     validation_data=validation_set,\n",
    "                     epochs=35)\n",
    "\n",
    "stop=time.time()\n",
    "time=str(stop-start)\n",
    "\n",
    "print(\"model3 requires \" +time+ \" training time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xU76TCZ0izA9"
   },
   "outputs": [],
   "source": [
    "list_ep = [i for i in range(1, 36)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['accuracy'], ls = '--', label = 'accuracy')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_accuracy'], ls = '--', label = 'val_accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.title(\"Model 3 History\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mO0RkYZMi3km"
   },
   "outputs": [],
   "source": [
    "list_ep = [i for i in range(1, 36)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['loss'], ls = '--', label = 'loss')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_loss'], ls = '--', label = 'val_loss')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.title(\"Model 3 Training History\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tE7-v2zhjA3d"
   },
   "outputs": [],
   "source": [
    "model3.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3D85pn3i72y"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix and generate a classification report for the model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "test_set = datagen_test.flow_from_directory(folder_path + \"test\",\n",
    "                                                              target_size = (img_size,img_size),\n",
    "                                                              color_mode = 'grayscale',\n",
    "                                                              batch_size = 128,\n",
    "                                                              class_mode = 'categorical',\n",
    "                                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                                              shuffle = True) \n",
    "test_images, test_labels = next(test_set)\n",
    "\n",
    "# Write the name of your chosen model in the blank\n",
    "pred = model3.predict(test_images)\n",
    "pred = np.argmax(pred, axis = 1) \n",
    "y_true = np.argmax(test_labels, axis = 1)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(y_true, pred))\n",
    "\n",
    "# Plotting the heatmap using confusion matrix\n",
    "cm = confusion_matrix(y_true, pred)\n",
    "\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize = (8, 5))\n",
    "sns.heatmap(cmn, annot = True,  fmt = '.2f', xticklabels = ['happy', 'sad', 'neutral', 'surprise'], yticklabels = ['happy', 'sad', 'neutral', 'surprise'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0vJIbhUgb-y"
   },
   "source": [
    "### **kernel size (3,3), learning rates 0.001**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAc_VjeXhrRn"
   },
   "outputs": [],
   "source": [
    "no_of_classes = 4\n",
    "  \n",
    "model3 = Sequential()\n",
    "\n",
    "# Add 1st CNN Block\n",
    "model3.add(Conv2D(64, (3,3), activation='relu', input_shape=(48, 48, 1), padding = 'same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(LeakyReLU(0.1))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# Add 2nd CNN Block\n",
    "model3.add(Conv2D(128, (3,3), activation='relu', padding = 'same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(LeakyReLU(0.1))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "# Add 3rd CNN Block\n",
    "model3.add(Conv2D(512, (3,3), activation='relu', padding = 'same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(LeakyReLU(0.1))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# Add 4th CNN Block\n",
    "model3.add(Conv2D(512, (3,3), activation='relu', padding = 'same'))\n",
    "\n",
    "# Add 5th CNN Block\n",
    "model3.add(Conv2D(128, (3,3), activation='relu', padding = 'same'))\n",
    "\n",
    "\n",
    "model3.add(Flatten())\n",
    "\n",
    "# First fully connected layer\n",
    "model3.add(Dense(256, activation=\"relu\"))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "# Second fully connected layer\n",
    "model3.add(Dense(512, activation=\"relu\"))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "model3.add(Dense(no_of_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMzCW3dShsaP"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "epochs = 35\n",
    "\n",
    "steps_per_epoch = train_set.n//train_set.batch_size\n",
    "validation_steps = validation_set.n//validation_set.batch_size\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model3.h5\", monitor = 'val_accuracy',\n",
    "                            save_weights_only = True, model = 'max', verbose = 1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 2, min_lr = 0.0001 , model = 'auto')\n",
    "\n",
    "callbacks = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52IT2QIMhuoM"
   },
   "outputs": [],
   "source": [
    "# Write your code to compile your model3. Use categorical crossentropy as the loss function, Adam Optimizer with 0.003 learning rate, and set metrics to 'accuracy'.\n",
    "model3.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "model3.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDwdm5_qhyTh"
   },
   "outputs": [],
   "source": [
    "# Write your code to fit your model. Use train_set as the training data and validation_set as the validation data. Train your model for 35 epochs.\n",
    "import time\n",
    "start=time.time()\n",
    "history = model3.fit(train_set,\n",
    "                     validation_data=validation_set,\n",
    "                     epochs=35)\n",
    "\n",
    "stop=time.time()\n",
    "time=str(stop-start)\n",
    "\n",
    "print(\"model3 requires \" +time+ \" training time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yj7f7e2Ah6XF"
   },
   "outputs": [],
   "source": [
    "list_ep = [i for i in range(1, 36)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['accuracy'], ls = '--', label = 'accuracy')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_accuracy'], ls = '--', label = 'val_accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.title(\"Model 3 History\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SWGB5SUh_h-"
   },
   "outputs": [],
   "source": [
    "list_ep = [i for i in range(1, 36)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['loss'], ls = '--', label = 'loss')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_loss'], ls = '--', label = 'val_loss')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.title(\"Model 3 Training History\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBOAW5buiBH3"
   },
   "outputs": [],
   "source": [
    "model3.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEblaqQ2iF1c"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix and generate a classification report for the model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "test_set = datagen_test.flow_from_directory(folder_path + \"test\",\n",
    "                                                              target_size = (img_size,img_size),\n",
    "                                                              color_mode = 'grayscale',\n",
    "                                                              batch_size = 128,\n",
    "                                                              class_mode = 'categorical',\n",
    "                                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                                              shuffle = True) \n",
    "test_images, test_labels = next(test_set)\n",
    "\n",
    "# Write the name of your chosen model in the blank\n",
    "pred = model3.predict(test_images)\n",
    "pred = np.argmax(pred, axis = 1) \n",
    "y_true = np.argmax(test_labels, axis = 1)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(y_true, pred))\n",
    "\n",
    "# Plotting the heatmap using confusion matrix\n",
    "cm = confusion_matrix(y_true, pred)\n",
    "\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize = (8, 5))\n",
    "sns.heatmap(cmn, annot = True,  fmt = '.2f', xticklabels = ['happy', 'sad', 'neutral', 'surprise'], yticklabels = ['happy', 'sad', 'neutral', 'surprise'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RIAjkGogfQI"
   },
   "source": [
    "### **kernel size (2,2), learning rates 0.001**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DP6_dGCygXuU"
   },
   "outputs": [],
   "source": [
    "no_of_classes = 4\n",
    "  \n",
    "model3 = Sequential()\n",
    "\n",
    "# Add 1st CNN Block\n",
    "model3.add(Conv2D(64, (2,2), activation='relu', input_shape=(48, 48, 1), padding = 'same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(LeakyReLU(0.1))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# Add 2nd CNN Block\n",
    "model3.add(Conv2D(128, (2,2), activation='relu', padding = 'same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(LeakyReLU(0.1))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "# Add 3rd CNN Block\n",
    "model3.add(Conv2D(512, (2,2), activation='relu', padding = 'same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(LeakyReLU(0.1))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# Add 4th CNN Block\n",
    "model3.add(Conv2D(512, (2,2), activation='relu', padding = 'same'))\n",
    "\n",
    "# Add 5th CNN Block\n",
    "model3.add(Conv2D(128, (2,2), activation='relu', padding = 'same'))\n",
    "\n",
    "\n",
    "model3.add(Flatten())\n",
    "\n",
    "# First fully connected layer\n",
    "model3.add(Dense(256, activation=\"relu\"))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "# Second fully connected layer\n",
    "model3.add(Dense(512, activation=\"relu\"))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "model3.add(Dense(no_of_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfJIusYifvRA"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "epochs = 35\n",
    "\n",
    "steps_per_epoch = train_set.n//train_set.batch_size\n",
    "validation_steps = validation_set.n//validation_set.batch_size\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model3.h5\", monitor = 'val_accuracy',\n",
    "                            save_weights_only = True, model = 'max', verbose = 1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 2, min_lr = 0.0001 , model = 'auto')\n",
    "\n",
    "callbacks = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzFZCzApfqbw"
   },
   "outputs": [],
   "source": [
    "# Write your code to compile your model3. Use categorical crossentropy as the loss function, Adam Optimizer with 0.003 learning rate, and set metrics to 'accuracy'.\n",
    "model3.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "model3.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elrgzoMthJDU"
   },
   "outputs": [],
   "source": [
    "# Write your code to fit your model. Use train_set as the training data and validation_set as the validation data. Train your model for 35 epochs.\n",
    "import time\n",
    "start=time.time()\n",
    "history = model3.fit(train_set,\n",
    "                     validation_data=validation_set,\n",
    "                     epochs=35)\n",
    "\n",
    "stop=time.time()\n",
    "time=str(stop-start)\n",
    "\n",
    "print(\"model3 requires \" +time+ \" training time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJaE_C-lhMzk"
   },
   "outputs": [],
   "source": [
    "list_ep = [i for i in range(1, 36)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['accuracy'], ls = '--', label = 'accuracy')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_accuracy'], ls = '--', label = 'val_accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.title(\"Model 3 History\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzu3GU-XhTVz"
   },
   "outputs": [],
   "source": [
    "list_ep = [i for i in range(1, 36)]\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "plt.plot(list_ep, history.history['loss'], ls = '--', label = 'loss')\n",
    "\n",
    "plt.plot(list_ep, history.history['val_loss'], ls = '--', label = 'val_loss')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.title(\"Model 3 Training History\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx52BPWjhUG4"
   },
   "outputs": [],
   "source": [
    "# Write your code to evaluate the model performance on the test set\n",
    "model3.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_CwFqKOhWM7"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix and generate a classification report for the model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "test_set = datagen_test.flow_from_directory(folder_path + \"test\",\n",
    "                                                              target_size = (img_size,img_size),\n",
    "                                                              color_mode = 'grayscale',\n",
    "                                                              batch_size = 128,\n",
    "                                                              class_mode = 'categorical',\n",
    "                                                              classes = ['happy', 'sad', 'neutral', 'surprise'],\n",
    "                                                              shuffle = True) \n",
    "test_images, test_labels = next(test_set)\n",
    "\n",
    "# Write the name of your chosen model in the blank\n",
    "pred = model3.predict(test_images)\n",
    "pred = np.argmax(pred, axis = 1) \n",
    "y_true = np.argmax(test_labels, axis = 1)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(y_true, pred))\n",
    "\n",
    "# Plotting the heatmap using confusion matrix\n",
    "cm = confusion_matrix(y_true, pred)\n",
    "\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize = (8, 5))\n",
    "sns.heatmap(cmn, annot = True,  fmt = '.2f', xticklabels = ['happy', 'sad', 'neutral', 'surprise'], yticklabels = ['happy', 'sad', 'neutral', 'surprise'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmjaGLK1hY0t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
